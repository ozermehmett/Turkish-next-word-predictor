{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "V28"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DiY625X1on9"
   },
   "source": [
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "turkish_stopwords = set(\"\"\"\n",
    "acaba ama ancak bence böyle böylece çünkü daha de defa değil eğer en hem her\n",
    "hiç için ile ise kim mi nasıl ne neden niçin nitekim oysa öyle yani şu veya\n",
    "ya ya da yahut ki da e da ki eğer gibi işte\n",
    "\"\"\".split())\n",
    "\n",
    "def process_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data = ' '.join(lines)\n",
    "    data = data.replace('\\n', ' ').replace('\\r', ' ').replace('\\ufeff', '')\n",
    "    data = data.replace('“', '').replace('”', '')\n",
    "\n",
    "    data = data.lower()\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation.replace('.', ''))\n",
    "    data = data.translate(translator)\n",
    "\n",
    "    data = data.replace(':', '.').replace(';', '.').replace('?', '.').replace('!', '.').replace('...', '.').replace('..', '.').replace('-', '.')\n",
    "\n",
    "    data = ' '.join(data.split())\n",
    "\n",
    "    words = data.split()\n",
    "    words = [word for word in words if word not in turkish_stopwords]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "data = process_file(\"fts1.txt\")\n",
    "data1 = process_file(\"calikusu.txt\")\n",
    "\n",
    "combined_data = data + \" \" + data1"
   ],
   "metadata": {
    "id": "V4kS8vmR1z7D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([combined_data])\n",
    "\n",
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ],
   "metadata": {
    "id": "lfdjAtzA2Bkz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sequence_data = tokenizer.texts_to_sequences([combined_data])[0]\n",
    "\n",
    "sequences = []\n",
    "sequence_length = 5\n",
    "\n",
    "for i in range(sequence_length, len(sequence_data)):\n",
    "    words = sequence_data[i-sequence_length:i+1]\n",
    "    sequences.append(words)\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]"
   ],
   "metadata": {
    "id": "KDmYcvPa2Cb-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2PgM6cB2Djk",
    "outputId": "6c3603ae-8950-4088-c0b3-9fc9359da1e2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ],
   "metadata": {
    "id": "3CoCvofQ2EjQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 50, input_length=3))\n",
    "# model.add(LSTM(256, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(256, kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=5))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cq4creCl2GNI",
    "outputId": "903ab316-22db-45b0-c5dd-77b9331a9e0c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ],
   "metadata": {
    "id": "ft5LuOj42bX1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "history = model.fit(X, y, epochs=35, callbacks=[early_stopping], validation_split=0.2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lz8VOnTj2eDn",
    "outputId": "6eb1f81d-5fde-4ddf-9c0a-3e4c9f4f498f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "XsM8845F7PEv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "-vMVQF4c7pag",
    "outputId": "9b9b1d55-1436-45b5-da0b-d5a3b72fcd0c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.save('model.h5')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tH5jci5yEvQS",
    "outputId": "d83d6d63-f413-4be7-c297-004bf909d94a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!cp 'model.h5' \"/content/drive/MyDrive/testt/\"\n",
    "!cp 'token.pkl' \"/content/drive/MyDrive/testt/\""
   ],
   "metadata": {
    "id": "f1qV4vw5FQ5W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "\n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "\n",
    "  return predicted_word"
   ],
   "metadata": {
    "id": "n4BtJV5fMFSq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def word_processing(text):\n",
    "  text = text.lower()\n",
    "  text = text.split(\" \")\n",
    "  text = text[-5:]\n",
    "  return text"
   ],
   "metadata": {
    "id": "t00fBBGhMHrE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text = 'Merhaba benim adım mehmet, bugün'\n",
    "\n",
    "Predict_Next_Words(model, tokenizer, word_processing(text))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "aIaAlLOCMS8s",
    "outputId": "5c3cd2c1-8d38-45ed-bec2-c7f2118cf180"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "seed_text = 'Kütüphanede saatlerce çalıştıktan sonra biraz'\n",
    "Predict_Next_Words(model, tokenizer, word_processing(seed_text))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "jhoUn-Jg9Bud",
    "outputId": "70817917-c7a6-4bc0-f521-34d531be4ce3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "4YpD2K5396Q3"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
